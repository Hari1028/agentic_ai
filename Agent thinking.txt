2. What is “spike” and “missing cadence” in practice?
A. Spike detection (value-based anomalies)

Intuitively: “This value looks unusually high/low compared to recent history.”

For each metric, over some time window (say last N readings):

Compute basic stats like:

average (mean)

standard deviation

maybe previous value

Then, flag a spike when for example:

Z-score rule:
- |value - mean| > K * std_dev (e.g. K = 3)

Percent-change rule:
- abs(value - previous_value) / previous_value > threshold
e.g. > 50%, > 200%, etc.

Your agent doesn’t need to be “ML heavy” for a POC. Simple rules are totally fine and actually easier to explain in natural language.
You can even define spike types:

"SPIKE_UP" – sudden jump upwards

"SPIKE_DOWN" – sudden drop

These get saved into the anomalies table with some detail like:

“Value 1050.0 is 4.2 standard deviations above mean 500.3 for last 50 readings.”


B. Missing cadence detection (time-based anomalies)

This is about gaps in time where you expected data but didn’t get it.

Using metric_cadence:

You expect a reading every X seconds.

Look at the time difference between consecutive metric_readings for the same metric_id.

If gap > expected_interval + allowed_delay, then:

mark an anomaly: "MISSING_CADENCE"

Example:

Metric expects data every 60 seconds

You see readings at 10:00:00, 10:01:00, 10:04:00

Gaps:

60s → okay

180s → bigger than 60 + tolerance → missing cadence

The agent can store details like:

“No reading for 180 seconds; expected interval is 60s with 30s tolerance.”

4. How I’d phase this POC (no code, just plan)

If you want a clean story:

Phase 1 – Schema + Synthetic data

Design the 5 tables above.

Generate synthetic readings for 2–3 systems, 5–8 metrics.

Intentionally create:

Some spikes (manual jumps)

Some gaps (missing timestamps)

Phase 2 – Rules + SQL thinking

Decide:

Window size for spike stats (like “last 50 readings” or “last 1 day”)

Thresholds (3σ rule, 50% change, etc.)

Tolerances for missing cadence

Think how each rule maps to:

“Read inputs from tables”

“Produce outputs → anomalies table”

Phase 3 – Agent behavior

Design the “conversation flows”:

“Run full anomaly sweep”

“Explain a specific anomaly”

“List anomalies by metric / system / type / severity”

This will help you later when you plug into Autogen or any LLM framework.




Good call — the idea of the data being updated constantly (or at regular cadence) is key to making the demo feel real and for the “agent” to act like a monitoring/observability system (similar to Grafana or Azure Monitor). Let’s sketch how to demo this (with your SQLite + agent setup) + reflect on how Grafana / Azure Monitor do it, so you can borrow ideas.

How to demo data-updates + anomaly-agent behaviour

Here’s a suggested workflow for the demo:

a) Data ingestion + cadence simulation

Have a process (script / cron job) that inserts new readings into your metric_readings table every X seconds/minutes (matching what your metric_cadence expects).

For the demo, you might simulate 2 modes:

Normal mode: readings every expected interval, values in “normal” range.

Anomaly mode: insert a sudden spike, or skip some readings to simulate missing cadence.

b) Agent run / analyze loop

Have the anomaly-agent scheduled (or manually triggered) every time interval (e.g., every minute).

On each run:

It fetches the latest readings since last run.

It checks for missing cadence (i.e., check timestamp gaps > expected interval).

It checks for value‐based anomalies (spikes) using your rules.

It writes anomalies to anomalies table.

c) Dashboard/Report for demo

Use either a simple UI (web page) or even a notebook/console to show:

Current metric values (time‐series)

Highlight when an anomaly was detected (e.g., show red dot, label).

Show missing cadence events (table of “metric X missing reading at time T”).

Show agent summary: “In last run: 2 spikes detected for metric A; 1 missing cadence for metric B”.

d) Switch between “normal” and “anomaly” modes in the demo

For example:

Run for 5 minutes with normal data → agent shows “no anomalies”.

Then deliberately inject a spike or skip readings → agent detects anomaly and you show the result.

You might even animate or fast‐forward: e.g. insert 100 readings quickly so the audience can observe baseline, then insert one big jump or one big gap.

e) Connect to your relational model

Because you have the relational schema (metrics, cadence settings, readings, anomalies) you can demonstrate:

How changing the expected_interval_seconds for a metric in metric_cadence affects missing cadence detection.

How changing the threshold for spike detection (maybe stored as metadata or just in code) affects sensitivity.

This shows the configurability and the relational nature of the model (rather than hard‐wired).

f) Agent explanation / narrative

One of the powerful parts: the agent should explain why it flagged something: e.g., “The value for metric ‘orders_per_min’ at 10:45:00 was 450, average for last 50 readings is 200, standard deviation is 50 → >3×σ so flagged as SPIKE_UP.”

For missing cadence: “The last two readings were at 11:02:00 and 11:09:30 → gap 450 s, expected interval 60 s + tolerance 30 s → flagged as MISSING_CADENCE.”

This gives you the feel of a monitoring/observability system rather than just a static script.

How Grafana & Azure Monitor do similar things & what you can borrow
Grafana

Grafana supports time‐series dashboards, alerting, and anomaly detection. For example:

Grafana Cloud supports forecasting & outlier detection, where the system learns an expected baseline and then alerts when metrics deviate. 
Grafana Labs
+2
Grafana Labs
+2

There is a demo/ framework (for example using PromQL) using upper and lower “bands” (mean ± kσ) to detect anomalies. 
GitHub
+1

What to borrow:

Baseline + deviation logic (mean/σ) for value spikes.

Visualizing anomalies clearly (colored bands, red markers).

Alerting/notification (even if demo only shows log/console).

Dashboard for normal vs anomaly view.

Azure Monitor & Fabric / ADF

Azure Monitor supports metrics and logs collection, routing, dashboards, alerts. For example, for Azure Data Factory (ADF) you can monitor pipelines, activities and set alerts. 
Microsoft Learn
+1

For Microsoft Fabric (data platform) you can monitor pipeline/dataflow runs, data lake storage, etc. 
3Cloud
+1

What to borrow:

The concept of cadence/gap detection: e.g., pipeline hasn't run, or expected output missing → missing cadence.

The idea of multiple resources (pipelines, transformations, storage) being monitored under a unified system—so you could simulate your multiple “systems/metrics” notion.

Use of routing/aggregation: while you use SQLite locally, the concept of collecting from multiple metrics and presenting in dashboard is the same.

Putting it together into your POC storyline

Here’s a storyline you could present in the demo:

“Here is our relational model in SQLite: tables for systems, metrics, cadence, readings, anomalies.”

“We have an ingestion process that simulates metric readings every minute for two systems: System A (orders), System B (API latency).”

“Our agent runs every minute, checks for missing readings (cadence rule) and value spikes (baseline rule).”

Scenario 1 (normal): show 10 minutes of normal values → no anomalies.

Scenario 2 (missing cadence): turn off insertion for 2 minutes → agent detects missing cadence for metric X → you show in the dashboard and anomalies table.

Scenario 3 (spike): keep cadence but at minute 11 insert a value of 1000 when baseline is ~200 → agent detects spike → dashboard highlights it, agent explains it.

Show how changing the expected_interval_seconds in metric_cadence or the threshold for spikes changes sensitivity (e.g., you make the tolerance narrower → more missing‐cadence alerts).

Summarize: This behaves similarly to Grafana/Azure Monitor in that you have time‐series data, baseline, alerts, cadence checks, dashboard view.





Alright, let’s design this properly as a system you can demo, not just a script. No code, just structure + flows + concrete examples you can later implement.

1. Components & Data Flow (how the whole thing works)

Think of your POC as 4 main blocks:

Relational DB (SQLite)

Tables: systems, metrics, metric_cadence, metric_readings, anomalies, (optional anomaly_runs).

Data Generator (simulated metrics)

A process that inserts rows into metric_readings at a fixed cadence.

Has “normal mode” and “anomaly mode”.

Anomaly Agent

Periodically runs (like a mini scheduler).

Reads from metric_readings + config tables.

Detects:

SPIKE / DROP (value anomalies)

MISSING_CADENCE (time gaps)

Writes to anomalies (and possibly anomaly_runs).

Dashboard / Viewer

Could be Streamlit, simple web app, or even a notebook.

Shows:

Metric charts (recent values)

Anomalies list

Agent’s textual summary.

ASCII-style flow diagram
       [Data Generator]  (runs every N seconds)
              |
              v
   +----------------------+
   |  SQLite Relational   |
   |  DB (VS Code)        |
   |                      |
   |  systems             |
   |  metrics             |
   |  metric_cadence      |
   |  metric_readings <---+  (inserted by generator)
   |  anomalies      <----+  (written by agent)
   +----------------------+
              ^
              |
        [Anomaly Agent]  (runs every M seconds)
              ^
              |
       [User / Dashboard]
          - view metrics
          - view anomalies
          - ask: "what happened?"


You’ll “sell” this as:

“This is like a mini Grafana/Azure Monitor built on SQLite + an anomaly agent.”

2. Minimal Metric Design for Demo

Pick 2 systems and 4 metrics so it’s rich but not overwhelming.

Systems

System A – Orders Service

system_name: "Orders Service"

System B – Payments API

system_name: "Payments API"

Metrics

For Orders Service:

orders_per_min

Unit: count/min

Expected: generally between 100–250 per minute.

order_error_rate

Unit: %

Expected: around 0–2%.

For Payments API:

api_latency_ms

Unit: ms

Expected: 150–300 ms typical.

failed_payments_per_min

Unit: count/min

Expected: 0–10 per minute.

Cadence Config (metric_cadence)

For all 4 metrics (for simplicity):

expected_interval_seconds: 60 (one reading per minute)

allowed_delay_seconds: 30 (up to 90 seconds gap is fine)

Monitoring window for agent: “last 30 mins” (for spikes).

3. Normal Operation Scenario (baseline)

This is the “everything is healthy” phase.

3.1. Data Generator behavior (Normal Mode)

Every minute (say from 10:00 to 10:20):

Insert one row per metric:

Example ranges:

orders_per_min: random between 150–220

e.g.: 10:00 → 180, 10:01 → 195, 10:02 → 170, …

order_error_rate: random between 0.2%–1.8%

e.g.: 10:00 → 0.8, 10:01 → 1.1, …

api_latency_ms: random between 180–260

e.g.: 10:00 → 200, 10:01 → 210, …

failed_payments_per_min: random between 1–8

e.g.: 10:00 → 3, 10:01 → 4, …

3.2. Anomaly Agent behavior in Normal Mode

Agent runs every minute (e.g. at 10:05, 10:06, …):

For each metric:

Fetch last N readings (say last 20 minutes).

Compute:

mean and std dev (σ) of values.

check last two timestamps for gap.

Apply rules:

Spike rule (example):

Flag if |latest - mean| > 3 * std_dev.

Missing cadence rule:

Let gap = latest_timestamp - previous_timestamp.

Flag if gap > expected_interval + allowed_delay (here > 90s).

During this phase, all metrics stay within normal ranges and exact 60-second cadence, so:

No rows in anomalies.

Dashboard shows “All clear”.

This phase proves:

“Our agent does NOT spam false alerts when the system behaves normally.”

4. Missing Cadence Scenario (like Azure Monitor alerting)

Now let’s inject a gap in data to simulate something like “pipeline stopped” or “data source down”.

4.1. Data Generator behavior (Missing Cadence)

Up to 10:20 – normal data.

Then:

From 10:21 to 10:25 – the generator stops writing for:

orders_per_min

order_error_rate

But continues writing for:

api_latency_ms

failed_payments_per_min

So for Orders Service metrics, you have:

Last reading at 10:20

Next reading only resumes at 10:26

This creates a gap:

gap = 10:26 - 10:20 = 6 minutes = 360 seconds

Given:

expected_interval = 60

allowed_delay = 30

Condition: gap > 90 seconds → true → missing cadence.

4.2. Agent behavior & “story”

Let’s say the agent runs at:

10:22 → sees 10:20 as last reading, no new one yet.

gap = 120 seconds (2 minutes) → already > 90 → can flag.

You can decide whether to:

Flag as soon as the breach happens, or

Flag once data resumes (seeing the gap). For demo, do it as soon as the breach happens—more dramatic.

Detected anomaly example:

In anomalies table:

metric_id: (for orders_per_min)

timestamp_utc: 10:22

anomaly_type: MISSING_CADENCE

severity: HIGH (because it hit 4× the interval)

details:

“No reading received for 120 seconds. Expected interval is 60s with 30s tolerance. Last reading at 10:20.”

Repeat similarly for order_error_rate.

How you explain it in the demo (like Azure Monitor):

“Just like Azure Monitor would alert when an ADF pipeline hasn’t run on schedule, our anomaly agent detected that the Orders metrics stopped arriving on their expected 1-minute cadence and raised a MISSING_CADENCE anomaly.”

5. Spike Scenario (Grafana-style anomaly)

Now you show a value spike while cadence is fine.

5.1. Data Generator behavior (Spike Mode)

Let’s use the Payments API latency metric api_latency_ms.

From 10:30 to 10:40:

Normal values: 190–230 ms.

At 10:41:

Insert one huge spike: api_latency_ms = 1200 ms.

Cadence remains 60s.

For last 20 minutes, you might have:

Mean ≈ 210 ms

σ ≈ 20–30 ms

Then:

difference = 1200 - 210 = 990

990 / 30 ≈ 33 → 33σ above mean → definitely a spike.

5.2. Agent behavior & anomaly record

Agent runs at 10:42, sees reading at 10:41.

Spike rule:

If |value - mean| > 3σ → anomaly.

It is, so you write:

metric_id: api_latency_ms

timestamp_utc: 10:41

anomaly_type: SPIKE_UP

severity: HIGH

details:

“Observed 1200 ms at 10:41. Baseline mean over last 20 readings is 210 ms, std dev is 30 ms. Deviation is 33.0σ above mean.”

How you explain in the demo (Grafana-like):

“Like a Grafana dashboard with outlier detection, our agent learned the normal latency (around 200 ms) and noticed a sudden spike to 1200 ms. It flagged this as a high-severity SPIKE_UP anomaly and recorded the statistical context.”

You can show:

Time series chart: nice line around 200, then tall spike at 10:41 highlighted in red.

Underneath: anomaly details in a table.

6. Combined Scenario (mega demo)

For a slick demo, you can chain the three:

Phase 1 – Baseline (first 10–15 minutes)

No anomalies → dashboard green.

Phase 2 – Missing cadence for Orders

Stop inserting Orders metrics for a few minutes.

Agent flags MISSING_CADENCE for:

orders_per_min

order_error_rate.

Phase 3 – Spike in Payments latency

Restore Orders metrics.

Inject a spike in api_latency_ms.

Agent flags SPIKE_UP for that metric.

Then, show a textual summary like:

“In the last 30 minutes:

Orders Service: 2 MISSING_CADENCE anomalies between 10:21–10:24.

Payments API: 1 SPIKE_UP anomaly at 10:41 with latency 1200 ms.”

This maps very nicely to:

Azure Monitor → pipeline/metric not running → missing cadence.

Grafana → time-series threshold/outlier → spikes.

7. Agent Logic – Step-by-step (for each run)

For each agent run (say every minute):

Identify time window

Example: “analyze last 30 minutes”.

Load configs

From metrics, metric_cadence for all metrics where active_flag = 1.

For each metric:

Fetch readings in window from metric_readings.

Sort by timestamp.

Missing cadence check:

Look at the last two timestamps.

Compute gap.

If gap > expected_interval + allowed_delay → write MISSING_CADENCE.

Spike check:

On last K readings (e.g. last 20):

compute mean & std dev.

compute z-score of last reading.

If |z| > spike_threshold (e.g. 3 or 4) → write SPIKE_UP or SPIKE_DOWN.

Write anomalies

Insert into anomalies.

Optionally insert an entry into anomaly_runs like:

run_id, run_started_at, run_ended_at, metrics_checked, anomalies_found.

Generate textual summary for UI

“This run: X anomalies (Y spikes, Z missing cadence) across N metrics.”